{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas Filtro:\n",
    "\n",
    "- Lenguaje Filtro (Python)\n",
    "- Crear sistema de web scraping, y crear dataset de al menos 5 categorías, con datos obtenidos a       través del web scraping.\n",
    "- Las categorías deben de estar numéricamente balanceadas en cantidad.\n",
    "- Se debe crear un procedimiento automatizado de preprocesamiento .\n",
    "- Se debe desarrollar un modelo de redes neuronales artificiales con tensor flow/keras , para el       procesamiento de este dataset.\n",
    "- Se debe crear una api rest , que proporcione por medio de métodos http , un ingreso de datos y una   salida de la clasificación del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actualizamos el inslalador pip e instalamos las biblotecas para hacer el scraping y acceder a la página a scrapear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip\n",
    "# ! pip install beautifulsoup4\n",
    "# ! pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos las 5 categorías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorías\n",
    "\n",
    "1. Edad antigua\n",
    "2. Edad media\n",
    "3. Edad moderna\n",
    "4. Edad contemporánea\n",
    "5. Prehistoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clases para periodismo\n",
    "\n",
    "- Política y economía\n",
    "- Ciencia, tecnología, salud, cultura, innovación y emprendedorismo\n",
    "- Viajes, turismo\n",
    "- Espectáculos, recreación, gastronomía y sociales\n",
    "- Deportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get # la vamos a usar para solicitar la página mediante su URL \n",
    "from bs4 import BeautifulSoup # biblioteca para web scraping\n",
    "import pandas as pd\n",
    "import string                              \n",
    "from nltk.corpus import stopwords # para remover palabras que no agregan significado          \n",
    "from nltk.stem import PorterStemmer # para llevar las palabras a su forma raíz        \n",
    "from nltk.tokenize import TweetTokenizer # para transformar cada palabra en un token\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "       with open(name + '.pkl', 'wb') as f:\n",
    "           pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "       with open(name + '.pkl', 'rb') as f:\n",
    "           return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clases = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1 - Scraping de textos crudos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "webs = pd.ExcelFile(\"Webs.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Categorias']\n"
     ]
    }
   ],
   "source": [
    "print(webs.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>NOMBRE ARCHIVO</th>\n",
       "      <th>CLASE</th>\n",
       "      <th>CÓDIGO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Alexander_the_Great</td>\n",
       "      <td>Alexander_the_Great</td>\n",
       "      <td>EDAD ANTIGUA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org/wiki/History_of_scien...</td>\n",
       "      <td>History_of_science_in_classical_antiquity</td>\n",
       "      <td>EDAD ANTIGUA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Roman_technology</td>\n",
       "      <td>Roman_technology</td>\n",
       "      <td>EDAD ANTIGUA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Boudica</td>\n",
       "      <td>Boudica</td>\n",
       "      <td>EDAD ANTIGUA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Julius_Caesar</td>\n",
       "      <td>Julius_Caesar</td>\n",
       "      <td>EDAD ANTIGUA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Prehistory_of_Au...</td>\n",
       "      <td>Prehistory_of_Australia</td>\n",
       "      <td>PREHISTORIA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Control_of_fire_...</td>\n",
       "      <td>Control_of_fire_by_early_humans</td>\n",
       "      <td>PREHISTORIA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Archaeology_of_t...</td>\n",
       "      <td>Archaeology_of_the_Americas</td>\n",
       "      <td>PREHISTORIA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Indigenous_Austr...</td>\n",
       "      <td>Indigenous_Australian_art</td>\n",
       "      <td>PREHISTORIA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Hunter-gatherer</td>\n",
       "      <td>Hunter-gatherer</td>\n",
       "      <td>PREHISTORIA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "0   https://en.wikipedia.org/wiki/Alexander_the_Great   \n",
       "1   https://en.wikipedia.org/wiki/History_of_scien...   \n",
       "2      https://en.wikipedia.org/wiki/Roman_technology   \n",
       "3               https://en.wikipedia.org/wiki/Boudica   \n",
       "4         https://en.wikipedia.org/wiki/Julius_Caesar   \n",
       "..                                                ...   \n",
       "84  https://en.wikipedia.org/wiki/Prehistory_of_Au...   \n",
       "85  https://en.wikipedia.org/wiki/Control_of_fire_...   \n",
       "86  https://en.wikipedia.org/wiki/Archaeology_of_t...   \n",
       "87  https://en.wikipedia.org/wiki/Indigenous_Austr...   \n",
       "88      https://en.wikipedia.org/wiki/Hunter-gatherer   \n",
       "\n",
       "                               NOMBRE ARCHIVO         CLASE  CÓDIGO  \n",
       "0                         Alexander_the_Great  EDAD ANTIGUA       1  \n",
       "1   History_of_science_in_classical_antiquity  EDAD ANTIGUA       1  \n",
       "2                            Roman_technology  EDAD ANTIGUA       1  \n",
       "3                                     Boudica  EDAD ANTIGUA       1  \n",
       "4                               Julius_Caesar  EDAD ANTIGUA       1  \n",
       "..                                        ...           ...     ...  \n",
       "84                    Prehistory_of_Australia   PREHISTORIA       5  \n",
       "85            Control_of_fire_by_early_humans   PREHISTORIA       5  \n",
       "86                Archaeology_of_the_Americas   PREHISTORIA       5  \n",
       "87                  Indigenous_Australian_art   PREHISTORIA       5  \n",
       "88                            Hunter-gatherer   PREHISTORIA       5  \n",
       "\n",
       "[89 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = webs.parse(webs.sheet_names[0])\n",
    "clases_df = set(df[\"CÓDIGO\"].value_counts().index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2d509d76da04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtextos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Texto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Clase'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"URL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "textos = pd.DataFrame(columns=['Texto', 'Clase'])\n",
    "for i in df.index: \n",
    "    print(\"i\", i)\n",
    "    url = df[\"URL\"][i]\n",
    "    req = requests.get(url)\n",
    "    statusCode = req.status_code\n",
    "    html = BeautifulSoup(req.text, \"html.parser\")\n",
    "    if statusCode == 200:\n",
    "        paragraphs = html.select(\"p\")\n",
    "        cont = 0\n",
    "        for para in paragraphs:\n",
    "            cont += 1 \n",
    "            parrafo = \"\"\n",
    "            print(\"parrafo\",parrafo) \n",
    "            print(\"i\",i) \n",
    "            print(\"df['CÓDIGO'][i]\",df[\"CÓDIGO\"][i]) \n",
    "            print('df[\"NOMBRE ARCHIVO\"][i]',df[\"NOMBRE ARCHIVO\"][i])\n",
    "            print(\"cont\",cont) \n",
    "            guardarComo = str(df[\"CÓDIGO\"][i]) + \"/\"  + df[\"NOMBRE ARCHIVO\"][i] + str(cont)\n",
    "            print(\"guardarComo\",guardarComo)  \n",
    "            parrafo = open(guardarComo,\"w\",encoding = \"utf-8\") \n",
    "            parrafo.write(para.text)\n",
    "            textos = textos.append({'Texto': para.text, 'Clase':df[\"CÓDIGO\"][i]}, ignore_index=True)\n",
    "            parrafo.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos[\"Texto\"][6177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos.to_csv(\"H01 - HistoriaTextosCrudos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2 - Preprocesamiento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosCrudos = pd.read_csv(\"H01 - HistoriaTextosCrudos.csv\")\n",
    "textosCrudos = textosCrudos.iloc[:,1:]\n",
    "textosCrudos.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosCrudos.groupby(by=[\"Clase\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosCrudos[\"TextoProcesado\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosCrudos.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "stop        = stopwords.words('english')\n",
    "tokenizer   = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "stemmer     = PorterStemmer()\n",
    "\n",
    "textos      = pd.DataFrame(columns = [ 'Texto' , 'Clase'])\n",
    "\n",
    "for i in textosCrudos.index:\n",
    "    cont += 1\n",
    "    if cont > 10:\n",
    "        break\n",
    "    print()    \n",
    "    texto_tokens = tokenizer.tokenize(textosCrudos[\"Texto\"][i])\n",
    "    texto_clean  = [ word for word in texto_tokens if word not in stop and word not in string.punctuation ]\n",
    "    texto_stem   = [ stemmer.stem(word) for word in texto_clean ]\n",
    "    for t in range(len(texto_stem)//20):\n",
    "        j = 20 * t\n",
    "        palabras  = texto_stem[ j : 20 * (t+1) ] \n",
    "        textos   = textos.append({'Texto' : palabras, 'Clase' : textosCrudos[\"Clase\"][i] }, ignore_index = True)\n",
    "    \n",
    "    print(textosCrudos[\"Texto\"][i])\n",
    "    print(texto_tokens)\n",
    "    print() \n",
    "    print (texto_clean)\n",
    "    print() \n",
    "    print (texto_stem)\n",
    "textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in textos.index:\n",
    "    print()\n",
    "    print(textos[\"Texto\"][i])\n",
    "textos    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop        = stopwords.words('english')\n",
    "tokenizer   = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "stemmer     = PorterStemmer()\n",
    "\n",
    "textos      = pd.DataFrame(columns = [ 'Texto' , 'Clase'])\n",
    "\n",
    "for i in textosCrudos.index: \n",
    "    texto_tokens = tokenizer.tokenize(textosCrudos[\"Texto\"][i])\n",
    "    texto_clean  = [ word for word in texto_tokens if word not in stop and word not in string.punctuation ]\n",
    "    texto_stem   = [ stemmer.stem(word) for word in texto_clean ]\n",
    "    for t in range(len(texto_stem) // 20):\n",
    "        j = 20 * t\n",
    "        palabras  = texto_stem[ j : 20 * (t+1) ] \n",
    "        textos   = textos.append({'Texto' : palabras, 'Clase' : textosCrudos[\"Clase\"][i] }, ignore_index = True)\n",
    "        clear_output( wait = True )\n",
    "        print(f\"Creando Textos {i} de {len(textosCrudos.index)}\")\n",
    "textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "for i in textos.index:\n",
    "    cont += 1\n",
    "    if cont > 10:\n",
    "        break\n",
    "    print()    \n",
    "    print(\"longitud:\",len(textos[\"Texto\"][i].split()), textos[\"Texto\"][i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos.to_csv(\"H02 - Historia.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulario y frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = pd.read_csv(\"H02 - Historia.csv\")\n",
    "textos = textos.iloc[:,1:]\n",
    "textos.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos.groupby(by=[\"Clase\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 58\n",
    "for m in textos.index:\n",
    "    if m < cont:\n",
    "        continue\n",
    "    cont +=1\n",
    "    if cont > 61:\n",
    "        break\n",
    "    print()    \n",
    "    print(m)    \n",
    "    print(type(textos.loc[m].Texto))        \n",
    "    print(textos.loc[m].Texto)    \n",
    "    print()\n",
    "    print(textos.loc[m].Texto.split(\",\"))  \n",
    "    lista = textos.loc[m].Texto.split(\",\")\n",
    "    cont2 = 0\n",
    "    print()\n",
    "    for word in lista:\n",
    "        cont2 += 1\n",
    "        if cont2 == len(lista):\n",
    "            word = word[:-1] \n",
    "        word = word[2:-1]     \n",
    "        print()\n",
    "        print(len(word),word,word[-1:],len(word[-1:]))\n",
    "        if word[-1:] == \"'\":\n",
    "            print(\"True\")\n",
    "            word = word[:-1]\n",
    "            print(\"queda:\",word)\n",
    "        else:\n",
    "            print(\"False\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = set() \n",
    "for m in textos.index:\n",
    "    lista = textos.loc[m].Texto.split(\",\")\n",
    "    cont2 = 0\n",
    "    for word in lista:\n",
    "        cont2 += 1\n",
    "        if cont2 == len(lista):\n",
    "            word = word[:-1] \n",
    "        word = word[2:-1]   \n",
    "        if word[-1:] == \"'\":\n",
    "            word = word[:-1]\n",
    "        vocabulario.add(word)\n",
    "        clear_output( wait = True )\n",
    "        print(f\"Creando Vocabulario {m} de {len(textos.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"hunter-gather\" in vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(vocabulario, \"Vocabulario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulario:\n",
    "    print(word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = load_obj(\"Vocabulario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias = defaultdict(int)\n",
    "cont = 0 \n",
    "for i in textos.index:\n",
    "    cont += 1\n",
    "    print()\n",
    "    print(\"Contador\",cont)\n",
    "    if cont > 10:\n",
    "        break\n",
    "    lista = textos.loc[m].Texto.split(\",\")\n",
    "    for word in lista:\n",
    "        cont2 += 1\n",
    "        if cont2 == len(lista):\n",
    "            word = word[:-1] \n",
    "        word = word[2:-1]   \n",
    "        if word[-1:] == \"'\":\n",
    "            word = word[:-1]\n",
    "        print(word)\n",
    "        # word = \"pirulito\"\n",
    "        assert (word in vocabulario),\"La palabra no está en el vocabulario!\"\n",
    "        frecuencias[(word, textos.loc[i].Clase)] += 1\n",
    "        # clear_output( wait = True )\n",
    "        # print(f\"Creando Frecuencias {i} de {len(textos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias = defaultdict(int)\n",
    "for i in textos.index:\n",
    "    lista = textos.loc[m].Texto.split(\",\")\n",
    "    for word in lista:\n",
    "        cont2 += 1\n",
    "        if cont2 == len(lista):\n",
    "            word = word[:-1] \n",
    "        word = word[2:-1]   \n",
    "        if word[-1:] == \"'\":\n",
    "            word = word[:-1]\n",
    "        print(word)\n",
    "        # word = \"pirulito\"\n",
    "        assert (word in vocabulario),\"La palabra no está en el vocabulario!\"\n",
    "        frecuencias[(word, textos.loc[i].Clase)] += 1\n",
    "        clear_output( wait = True )\n",
    "        print(f\"Creando Frecuencias {i} de {len(textos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(frecuencias, \"Frecuencias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias = load_obj(\"Frecuencias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frecuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiarWord(word):\n",
    "    word = word[:-1]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequency dictionary\n",
    "freqs = {}\n",
    "cont = 0\n",
    "for i in textos.index:  \n",
    "    cont += 1\n",
    "    if cont > 100000:\n",
    "        break\n",
    "    print()    \n",
    "    print(i)    \n",
    "    words = textos.loc[i,'Texto'].split(\",\")\n",
    "    clase = textos.loc[i,'Clase']\n",
    "    cont2 = 0\n",
    "    for word in words:\n",
    "        cont2 += 1\n",
    "        if cont2 == len(words):\n",
    "            word = word[:-1] \n",
    "        word = word[2:-1]   \n",
    "\n",
    "      \n",
    "        while word[-1:] == \"'\" or word[-1:] == '\"':\n",
    "            word = limpiarWord(word)\n",
    "        \n",
    "        print(word)\n",
    "        # word = \"pirulito\"\n",
    "        assert (word in vocabulario),\"La palabra \" + word + \"no está en el vocabulario!\"\n",
    "        pair = (word, clase)\n",
    "        freqs[pair] = freqs.get(pair, clase) + 1\n",
    "# check data type\n",
    "print(f'type(freqs) = {type(freqs)}')\n",
    "\n",
    "# check length of the dictionary\n",
    "print(f'len(freqs) = {len(freqs)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textos[\"Texto\"][60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textosCrudos[\"Texto\"][30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate V, the number of unique words in the vocabulary\n",
    "vocab = set([pair[0] for pair in freqs.keys()])\n",
    "V = len(vocab)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(freqs, \"Frecuencias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias = load_obj(\"Frecuencias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frecuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(words, freqs, vocabulario):\n",
    "    '''\n",
    "    Input: \n",
    "        words: a list of words for one input\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,6)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = words\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 6)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    cont2 = 0\n",
    "    for word in word_l:\n",
    "        cont2 += 1\n",
    "        if cont2 == len(words):\n",
    "            word = word[:-1] \n",
    "        word = word[2:-1]   \n",
    "        while word[-1:] == \"'\" or word[-1:] == '\"':\n",
    "            word = limpiarWord(word)\n",
    "        print(word)\n",
    "        assert (word in vocabulario),\"La palabra \" + word + \"no está en el vocabulario!\"\n",
    "\n",
    "        # increment the word count for the positive label 1\n",
    "        for i in range(1,6):\n",
    "            x[0,i] += freqs.get((word, i),0)        \n",
    "    assert(x.shape == (1, 6))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosFreq = textos.copy()\n",
    "for j in range(1, nclases):\n",
    "    textosFreq[\"Clase\" + str(j)] = None\n",
    "textosFreq    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in textos.index:\n",
    "    words = textos.loc[:,\"Texto\"][i].split(\",\")    \n",
    "    clases = extract_features(words, freqs, vocabulario)\n",
    "    nclases = len(clases[0]) \n",
    "    for j in range(1, nclases):\n",
    "        textosFreq[\"Clase\" + str(j)][i] = clases[0,j]\n",
    "textosFreq        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosFreq.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosFreq.to_csv(\"H03 - HistoriaFreq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuenciasMaximas = {} \n",
    "for word in vocabulario:    \n",
    "    freqWord = []\n",
    "    print(word)\n",
    "    for k in range(clases + 1):\n",
    "        freqWord.append(0)   \n",
    "    for k in range(1, len(freqWord)):\n",
    "        freqWord[k] = frecuencias.get((word, k),0)    \n",
    "    for k in range(1, len(freqWord)):\n",
    "        frecuenciasMaximas[word] = np.argmax(freqWord)   \n",
    "clasesPalabrasFM = {}\n",
    "for k in range(1,clases + 1):\n",
    "    clasesPalabrasFM[k] = []     \n",
    "for word in frecuenciasMaximas.keys():\n",
    "    clasesPalabrasFM[frecuenciasMaximas[word]].append(word)\n",
    "totV = 0        \n",
    "\n",
    "for k in range(1, clases + 1):\n",
    "    print()\n",
    "    print(\"Clase:\",k)\n",
    "    print(len(clasesPalabrasFM[k]))\n",
    "    totV += len(clasesPalabrasFM[k])  \n",
    "assert(totV == len(vocabulario)),\"PALABRAS FALTANTES\"    \n",
    "    \n",
    "frecuenciasMaxDetalle = {}     \n",
    "    \n",
    "for word, value in frecuenciasMaximas.items():\n",
    "    freqWord = []\n",
    "    for k in range(clases + 1):\n",
    "        freqWord.append(0)     \n",
    "    for k in range(1, len(freqWord)):\n",
    "        freqWord[k] = frecuencias.get((word, k),0)        \n",
    "    \n",
    "    if sum(freqWord) == frecuencias[word, np.argmax(freqWord)]:\n",
    "        # print(\"SOLO APARECE EN UNA CLASE\")\n",
    "        rate = 99\n",
    "    else:    \n",
    "        rate = int(100 * frecuencias[word, np.argmax(freqWord)] / sum(freqWord))\n",
    "    \n",
    "    frecuenciasMaxDetalle[word] = value, rate - 19\n",
    "    \n",
    "print()\n",
    "# print(frecuenciasMaxDetalle)\n",
    "\n",
    "\n",
    "print()\n",
    "print(len(frecuenciasMaxDetalle))\n",
    "for i in frecuenciasMaxDetalle.items():\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "for word in vocabulario:\n",
    "    print()\n",
    "    print(word)\n",
    "    freqWord = []\n",
    "    for k in range(clases + 1):\n",
    "        freqWord.append(0)   \n",
    "    for k in range(1, len(freqWord)):   \n",
    "        freqWord[k] = frecuencias.get((word, k), 0)    \n",
    "    print(freqWord)        \n",
    "    print(frecuenciasMaxDetalle[word])\n",
    "    print(\"Clase a ponderar\",np.argmax(freqWord))\n",
    "    print(\"Valor a ponderar\",freqWord[np.argmax(freqWord)])\n",
    "    print(\"Factor de ponderacion\",frecuenciasMaxDetalle[word][1])\n",
    "    print(\"Valor a Ponderar\", frecuencias[(word, np.argmax(freqWord))]  )\n",
    "    print(\"Factor de ponderacion\", (frecuenciasMaxDetalle[word][1] + 100) / 100) \n",
    "    print(\"Valor Pond.\",(int (frecuencias[(word, np.argmax(freqWord))]  * (frecuenciasMaxDetalle[word][1] + 100) / 100)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias[('panina', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "for key in frecuenciasMaximas.keys():\n",
    "    cont += 1\n",
    "    if cont > 10:\n",
    "        break\n",
    "    print()\n",
    "    print(key, frecuenciasMaximas[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
